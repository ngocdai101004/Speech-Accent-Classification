{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8319166,"sourceType":"datasetVersion","datasetId":4858283}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n","metadata":{"_uuid":"02d04162-f91d-4262-b722-2abd7298c589","_cell_guid":"dff94c6a-3b9a-45d0-af0f-2e2aeab35415","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-05T08:10:50.780491Z","iopub.execute_input":"2024-05-05T08:10:50.780976Z","iopub.status.idle":"2024-05-05T08:10:51.191721Z","shell.execute_reply.started":"2024-05-05T08:10:50.780939Z","shell.execute_reply":"2024-05-05T08:10:51.190368Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Prepare training data from Metadata file","metadata":{"_uuid":"d7e6bcc5-a1d3-44a1-8a64-2af00742ad40","_cell_guid":"4a58b9f8-dccd-40df-8ab5-c63d0e222bc3","trusted":true}},{"cell_type":"code","source":"from pathlib import Path","metadata":{"_uuid":"dac5e52d-0658-48aa-8d02-0bd21c4ebc76","_cell_guid":"a110aa0c-58e8-4984-8a4b-845cbe88c2ac","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-05T08:10:53.357728Z","iopub.execute_input":"2024-05-05T08:10:53.358473Z","iopub.status.idle":"2024-05-05T08:10:53.362752Z","shell.execute_reply.started":"2024-05-05T08:10:53.358438Z","shell.execute_reply":"2024-05-05T08:10:53.361827Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# ----------------------------\n# Prepare training data from Metadata file\n# ----------------------------\n\ndata_path = '/kaggle/input/vietnamese-speech-labeled-by-region/dataset/dataset'\n\n# Read metadata file\nmetadata_file = '/kaggle/input/vietnamese-speech-labeled-by-region/VNspeech.csv'\n\ndf = pd.read_csv(metadata_file)\n\ndf['relative_path'] = '/0' + df['region_fold'].astype(str) + '/0' + df['subregion_fold'].astype(str) + '/' + df['filename'].astype(str)\n\n# Take relevant columns\n# df = df[['relative_path', 'classID']]\nprint(df.head())","metadata":{"_uuid":"d85c205a-8fd8-49d7-9a9b-363d8a4bb781","_cell_guid":"e0221027-78a2-449e-854e-54e8d12eb972","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-05T08:10:55.012524Z","iopub.execute_input":"2024-05-05T08:10:55.012972Z","iopub.status.idle":"2024-05-05T08:10:55.074143Z","shell.execute_reply.started":"2024-05-05T08:10:55.012935Z","shell.execute_reply":"2024-05-05T08:10:55.073189Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"       filename  region_fold  subregion_fold  classID  class_name  \\\n0  06010001.wav            6             601        6  South West   \n1  06010002.wav            6             601        6  South West   \n2  06010003.wav            6             601        6  South West   \n3  06010004.wav            6             601        6  South West   \n4  06010005.wav            6             601        6  South West   \n\n           relative_path  \n0  /06/0601/06010001.wav  \n1  /06/0601/06010002.wav  \n2  /06/0601/06010003.wav  \n3  /06/0601/06010004.wav  \n4  /06/0601/06010005.wav  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load .wav data","metadata":{"_uuid":"c43fe6a6-dfc9-4682-845f-90cd6043c595","_cell_guid":"5a18acd9-5d6e-403f-888b-d493307c199d","trusted":true}},{"cell_type":"code","source":"import math, random\nimport torch\nimport torchaudio\nfrom torchaudio import transforms\nfrom IPython.display import Audio","metadata":{"_uuid":"e9649e2e-a11e-4ace-83d7-74c8b4120921","_cell_guid":"d02448a5-dcb8-434e-a0e0-a5727c003e17","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-05T08:10:57.778824Z","iopub.execute_input":"2024-05-05T08:10:57.779572Z","iopub.status.idle":"2024-05-05T08:11:02.510400Z","shell.execute_reply.started":"2024-05-05T08:10:57.779543Z","shell.execute_reply":"2024-05-05T08:11:02.509539Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class AudioUtil():\n   # ----------------------------\n   # Load an audio file. Return the signal as a tensor and the sample rate\n   # ----------------------------\n    @staticmethod\n    def open(audio_file):\n        sig, sr = torchaudio.load(audio_file)\n        return (sig, sr)\n\n\n    # ----------------------------\n    # Convert to two channels\n    # ----------------------------\n    # Convert the given audio to the desired number of channels\n    # ----------------------------\n    @staticmethod\n    def rechannel(aud, new_channel):\n        sig, sr = aud\n\n        if (sig.shape[0] == new_channel):\n          # Nothing to do\n            return aud\n\n        if (new_channel == 1):\n          # Convert from stereo to mono by selecting only the first channel\n            resig = sig[:1, :]\n        else:\n          # Convert from mono to stereo by duplicating the first channel\n            resig = torch.cat([sig, sig])\n            \n        return ((resig, sr))\n    \n    \n    # ----------------------------\n    # Standardize sampling rate\n    # ----------------------------\n    # Since Resample applies to a single channel, we resample one channel at a time\n    # ----------------------------\n    @staticmethod\n    def resample(aud, newsr):\n        sig, sr = aud\n\n        if (sr == newsr):\n            # Nothing to do\n            return aud\n\n        num_channels = sig.shape[0]\n        # Resample first channel\n        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n        if (num_channels > 1):\n            # Resample the second channel and merge both channels\n            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n            resig = torch.cat([resig, retwo])\n\n        return ((resig, newsr))\n    \n\n    # ----------------------------\n    # Resize to the same length\n    # ----------------------------\n    # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n    # ----------------------------\n    @staticmethod\n    def pad_trunc(aud, max_ms):\n        sig, sr = aud\n        num_rows, sig_len = sig.shape\n        max_len = sr//1000 * max_ms\n\n        if (sig_len > max_len):\n            # Truncate the signal to the given length\n            sig = sig[:,:max_len]\n\n        elif (sig_len < max_len):\n            # Length of padding to add at the beginning and end of the signal\n            pad_begin_len = random.randint(0, max_len - sig_len)\n            pad_end_len = max_len - sig_len - pad_begin_len\n\n            # Pad with 0s\n            pad_begin = torch.zeros((num_rows, pad_begin_len))\n            pad_end = torch.zeros((num_rows, pad_end_len))\n\n            sig = torch.cat((pad_begin, sig, pad_end), 1)\n   \n        return (sig, sr)\n    \n    # ----------------------------\n    # Data Augmentation: Time Shift\n    # ----------------------------\n    # Shifts the signal to the left or right by some percent. Values at the end\n    # are 'wrapped around' to the start of the transformed signal.\n    # ----------------------------\n    @staticmethod\n    def time_shift(aud, shift_limit):\n        sig,sr = aud\n        _, sig_len = sig.shape\n        shift_amt = int(random.random() * shift_limit * sig_len)\n        return (sig.roll(shift_amt), sr)\n    \n\n    # ----------------------------\n    # Convert to Mel Spectrogram\n    # ----------------------------\n    # Generate a Spectrogram\n    # ----------------------------\n    @staticmethod\n    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n        sig,sr = aud\n        top_db = 80 \n        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)   \n        # Convert to decibels\n        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n        return (spec)\n    \n    # ----------------------------\n    # Data Augmentation: Time and Frequency Masking\n    # ----------------------------\n    # Augment the Spectrogram by masking out some sections of it in both the frequency\n    # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n    # overfitting and to help the model generalise better. The masked sections are\n    # replaced with the mean value.\n    # ----------------------------\n    @staticmethod\n    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n        _, n_mels, n_steps = spec.shape\n        mask_value = spec.mean()\n        aug_spec = spec \n        freq_mask_param = max_mask_pct * n_mels\n        for _ in range(n_freq_masks):\n            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)    \n        time_mask_param = max_mask_pct * n_steps\n        for _ in range(n_time_masks):\n            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value) \n        return aug_spec","metadata":{"_uuid":"7a54fbeb-8d3c-4d06-8732-2cf4afbd6db4","_cell_guid":"ba65ad85-f451-4edf-b72a-5859679ce77c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-05T08:11:04.547196Z","iopub.execute_input":"2024-05-05T08:11:04.548162Z","iopub.status.idle":"2024-05-05T08:11:04.566670Z","shell.execute_reply.started":"2024-05-05T08:11:04.548130Z","shell.execute_reply":"2024-05-05T08:11:04.565624Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Dataset and DataLoader","metadata":{"_uuid":"fbb8dc8f-891a-4afa-96c2-f98ac7eebed6","_cell_guid":"1f00c74a-f8c2-4995-8b6e-8f11763156ae","trusted":true}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset, random_split\nimport torchaudio","metadata":{"_uuid":"6dbd425b-17fd-4df1-aae4-a38d0c58e4c5","_cell_guid":"500bd770-5196-4377-9c48-4b639d6f6d14","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-05T08:11:05.514872Z","iopub.execute_input":"2024-05-05T08:11:05.515760Z","iopub.status.idle":"2024-05-05T08:11:05.520064Z","shell.execute_reply.started":"2024-05-05T08:11:05.515726Z","shell.execute_reply":"2024-05-05T08:11:05.519133Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# ----------------------------\n# Sound Dataset\n# ----------------------------\nclass SoundDS(Dataset):\n    def __init__(self, df, data_path):\n        self.df = df\n        self.data_path = str(data_path)\n        self.duration = 4000\n        self.sr = 44100\n        self.channel = 2\n        self.shift_pct = 0.4\n            \n    # ----------------------------\n    # Number of items in dataset\n    # ----------------------------\n    def __len__(self):\n        return len(self.df)    \n      \n    # ----------------------------\n    # Get i'th item in dataset\n    # ----------------------------\n    def __getitem__(self, idx):\n        # Absolute file path of the audio file - concatenate the audio directory with\n        # the relative path\n        audio_file = self.data_path + self.df.loc[idx, 'relative_path']\n        # Get the Class ID\n        class_id = self.df.loc[idx, 'classID']\n    \n        aud = AudioUtil.open(audio_file)\n        # Some sounds have a higher sample rate, or fewer channels compared to the\n        # majority. So make all sounds have the same number of channels and same \n        # sample rate. Unless the sample rate is the same, the pad_trunc will still\n        # result in arrays of different lengths, even though the sound duration is\n        # the same.\n        reaud = AudioUtil.resample(aud, self.sr)\n        rechan = AudioUtil.rechannel(reaud, self.channel)\n    \n        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n    \n        return aug_sgram, class_id","metadata":{"_uuid":"3ea843f7-be65-4a0e-a9eb-59caa5a11786","_cell_guid":"84d1344c-ed18-4b01-84b1-113903d46377","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-05T08:11:07.103688Z","iopub.execute_input":"2024-05-05T08:11:07.104553Z","iopub.status.idle":"2024-05-05T08:11:07.114853Z","shell.execute_reply.started":"2024-05-05T08:11:07.104514Z","shell.execute_reply":"2024-05-05T08:11:07.113748Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import random_split","metadata":{"_uuid":"7b5c9b32-3e19-4130-ae91-12f32b0388f0","_cell_guid":"165abf3a-362e-49b3-8103-821279b50b9c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-05T08:11:08.235085Z","iopub.execute_input":"2024-05-05T08:11:08.235716Z","iopub.status.idle":"2024-05-05T08:11:08.240027Z","shell.execute_reply.started":"2024-05-05T08:11:08.235677Z","shell.execute_reply":"2024-05-05T08:11:08.238928Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"myds = SoundDS(df, data_path)\n\n# Random split of 80:20 between training and validation\nnum_items = len(myds)\nnum_train = round(num_items * 0.8)\nnum_val = num_items - num_train\ntrain_ds, val_ds = random_split(myds, [num_train, num_val])\n\n# Create training and validation data loaders\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\nval_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)","metadata":{"_uuid":"3ae88d9c-1b2a-42ab-92d0-c4992c43508c","_cell_guid":"1a8b8e35-5565-4854-9cc7-4a4b6f8a3210","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-05T08:11:09.699360Z","iopub.execute_input":"2024-05-05T08:11:09.700073Z","iopub.status.idle":"2024-05-05T08:11:09.721174Z","shell.execute_reply.started":"2024-05-05T08:11:09.700037Z","shell.execute_reply":"2024-05-05T08:11:09.720455Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"_uuid":"cd0ff235-4bcc-4f56-a2e3-d8d2f9d2df7c","_cell_guid":"88cd81b0-c37c-4261-8eec-e458a6ef95df","trusted":true}},{"cell_type":"code","source":"import torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.nn import init","metadata":{"_uuid":"198eff2b-fab9-49ee-bd77-a718562d0072","_cell_guid":"63854303-7fef-4cc8-8fb4-1adf6c56d27c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-05T08:11:12.014851Z","iopub.execute_input":"2024-05-05T08:11:12.015192Z","iopub.status.idle":"2024-05-05T08:11:12.019966Z","shell.execute_reply.started":"2024-05-05T08:11:12.015164Z","shell.execute_reply":"2024-05-05T08:11:12.018961Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# ----------------------------\n# Audio Classification Model\n# ----------------------------\nclass AudioClassifier (nn.Module):\n    # ----------------------------\n    # Build the model architecture\n    # ----------------------------\n    def __init__(self):\n        super().__init__()\n        conv_layers = []\n\n        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n        self.relu1 = nn.ReLU()\n        self.bn1 = nn.BatchNorm2d(8)\n        init.kaiming_normal_(self.conv1.weight, a=0.1)\n        self.conv1.bias.data.zero_()\n        conv_layers += [self.conv1, self.relu1, self.bn1]\n\n        # Second Convolution Block\n        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu2 = nn.ReLU()\n        self.bn2 = nn.BatchNorm2d(16)\n        init.kaiming_normal_(self.conv2.weight, a=0.1)\n        self.conv2.bias.data.zero_()\n        conv_layers += [self.conv2, self.relu2, self.bn2]\n\n        # Second Convolution Block\n        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu3 = nn.ReLU()\n        self.bn3 = nn.BatchNorm2d(32)\n        init.kaiming_normal_(self.conv3.weight, a=0.1)\n        self.conv3.bias.data.zero_()\n        conv_layers += [self.conv3, self.relu3, self.bn3]\n\n        # Second Convolution Block\n        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu4 = nn.ReLU()\n        self.bn4 = nn.BatchNorm2d(64)\n        init.kaiming_normal_(self.conv4.weight, a=0.1)\n        self.conv4.bias.data.zero_()\n        conv_layers += [self.conv4, self.relu4, self.bn4]\n\n        # Linear Classifier\n        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n        self.lin = nn.Linear(in_features=64, out_features=10)\n\n        # Wrap the Convolutional Blocks\n        self.conv = nn.Sequential(*conv_layers)\n \n    # ----------------------------\n    # Forward pass computations\n    # ----------------------------\n    def forward(self, x):\n        # Run the convolutional blocks\n        x = self.conv(x)\n\n        # Adaptive pool and flatten for input to linear layer\n        x = self.ap(x)\n        x = x.view(x.shape[0], -1)\n\n        # Linear layer\n        x = self.lin(x)\n\n        # Final output\n        return x\n\n# Create the model \nmodel = AudioClassifier()","metadata":{"_uuid":"6f0332ae-fe33-4785-be63-0fd99fc1629c","_cell_guid":"f2f0be67-bdf9-401d-8ace-91afab1ab33c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-05T08:11:15.358136Z","iopub.execute_input":"2024-05-05T08:11:15.358855Z","iopub.status.idle":"2024-05-05T08:11:15.414290Z","shell.execute_reply.started":"2024-05-05T08:11:15.358821Z","shell.execute_reply":"2024-05-05T08:11:15.413569Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"os.environ['TORCH_USE_CUDA_DSA'] = '1'\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n# Check that it is on Cuda\nnext(model.parameters()).device","metadata":{"_uuid":"8527f3ee-3e13-487e-956b-31658c2f39ae","_cell_guid":"5abe0d77-bd56-467d-8d5f-6986aa2f03c9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-05T08:11:19.185494Z","iopub.execute_input":"2024-05-05T08:11:19.186182Z","iopub.status.idle":"2024-05-05T08:11:19.434047Z","shell.execute_reply.started":"2024-05-05T08:11:19.186152Z","shell.execute_reply":"2024-05-05T08:11:19.433119Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Trainning","metadata":{"_uuid":"813b8eaa-180c-4040-bc44-37b20decf27b","_cell_guid":"832f23cb-ca18-437f-a075-23ea82dbd1e0","trusted":true}},{"cell_type":"code","source":"#from torch.utils.tensorboard import SummaryWriter","metadata":{"_uuid":"5844c22e-a338-4350-b158-af5d58721a28","_cell_guid":"18318aa5-2c92-4e8c-8b36-27ec204e83b0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-05T08:11:23.008924Z","iopub.execute_input":"2024-05-05T08:11:23.009740Z","iopub.status.idle":"2024-05-05T08:11:23.013449Z","shell.execute_reply.started":"2024-05-05T08:11:23.009697Z","shell.execute_reply":"2024-05-05T08:11:23.012452Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# ----------------------------\n# Training Loop\n# ----------------------------\ndef training(model, train_dl, num_epochs):\n    # Loss Function, Optimizer and Scheduler\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n                                                steps_per_epoch=int(len(train_dl)),\n                                                epochs=num_epochs,\n                                                anneal_strategy='linear')\n\n    # Repeat for each epoch\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct_prediction = 0\n        total_prediction = 0\n\n        # Repeat for each batch in the training set\n        for i, data in enumerate(train_dl):\n            # Get the input features and target labels, and put them on the GPU\n            inputs, labels = data[0].to(device), data[1].to(device)\n\n            # Normalize the inputs\n            inputs_m, inputs_s = inputs.mean(), inputs.std()\n            inputs = (inputs - inputs_m) / inputs_s\n\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            # Keep stats for Loss and Accuracy\n            running_loss += loss.item()\n\n            # Get the predicted class with the highest score\n            _, prediction = torch.max(outputs,1)\n            # Count of predictions that matched the target label\n            correct_prediction += (prediction == labels).sum().item()\n            total_prediction += prediction.shape[0]\n            #if i % 10 == 0:    # print every 10 mini-batches\n            #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n\n        # Print stats at the end of the epoch\n        num_batches = len(train_dl)\n        avg_loss = running_loss / num_batches\n        avg_acc = correct_prediction/total_prediction\n        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {avg_acc:.2f}')\n\n    print('Finished Training')","metadata":{"_uuid":"1bfe1bef-8a0c-4e61-9d56-707a8d5d8406","_cell_guid":"25af269b-919a-4a36-b48e-4a8918ee63cb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-05T08:11:23.835142Z","iopub.execute_input":"2024-05-05T08:11:23.835819Z","iopub.status.idle":"2024-05-05T08:11:23.846563Z","shell.execute_reply.started":"2024-05-05T08:11:23.835787Z","shell.execute_reply":"2024-05-05T08:11:23.845501Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## train with 100 epochs","metadata":{"_uuid":"f05bb060-b438-40c0-a966-1d8d116a46dc","_cell_guid":"d1ddb54a-ce4b-4228-905e-938f8f0e3a7c","trusted":true}},{"cell_type":"code","source":"num_epochs=10\ntraining(model, train_dl, num_epochs)\n\n","metadata":{"_uuid":"9ee6467f-6fe7-488f-98bd-fe3728cb7160","_cell_guid":"975ba0d0-be33-468a-b565-2989511ab7de","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-05T08:25:29.962153Z","iopub.execute_input":"2024-05-05T08:25:29.962782Z","iopub.status.idle":"2024-05-05T08:50:24.739961Z","shell.execute_reply.started":"2024-05-05T08:25:29.962751Z","shell.execute_reply":"2024-05-05T08:50:24.738844Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Epoch: 0, Loss: 0.82, Accuracy: 0.68\nEpoch: 1, Loss: 0.74, Accuracy: 0.70\nEpoch: 2, Loss: 0.68, Accuracy: 0.73\nEpoch: 3, Loss: 0.63, Accuracy: 0.74\nEpoch: 4, Loss: 0.59, Accuracy: 0.76\nEpoch: 5, Loss: 0.56, Accuracy: 0.77\nEpoch: 6, Loss: 0.55, Accuracy: 0.78\nEpoch: 7, Loss: 0.52, Accuracy: 0.79\nEpoch: 8, Loss: 0.50, Accuracy: 0.80\nEpoch: 9, Loss: 0.50, Accuracy: 0.80\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"try:\n    torch.save(model.state_dict(), '/kaggle/working/model.pt')\nexcept:\n    print(\"r\")","metadata":{"_uuid":"5a8a5aa7-0a64-4061-a1f3-1710f944154e","_cell_guid":"c8e82d1d-04ee-478f-92af-815894bb8826","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-05T08:59:28.607017Z","iopub.execute_input":"2024-05-05T08:59:28.607407Z","iopub.status.idle":"2024-05-05T08:59:28.618141Z","shell.execute_reply.started":"2024-05-05T08:59:28.607376Z","shell.execute_reply":"2024-05-05T08:59:28.617251Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# ----------------------------\n# Inference\n# ----------------------------\ndef inference (model, test_dl):\n    correct_prediction = 0\n    total_prediction = 0\n\n    # Disable gradient updates\n    with torch.no_grad():\n        for data in test_dl:\n            # Get the input features and target labels, and put them on the GPU\n            inputs, labels = data[0].to(device), data[1].to(device)\n\n            # Normalize the inputs\n            inputs_m, inputs_s = inputs.mean(), inputs.std()\n            inputs = (inputs - inputs_m) / inputs_s\n\n            # Get predictions\n            outputs = model(inputs)\n\n            # Get the predicted class with the highest score\n            _, prediction = torch.max(outputs,1)\n            # Count of predictions that matched the target label\n            correct_prediction += (prediction == labels).sum().item()\n            total_prediction += prediction.shape[0]\n        \n    acc = correct_prediction/total_prediction\n    print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T08:56:31.579306Z","iopub.execute_input":"2024-05-05T08:56:31.580111Z","iopub.status.idle":"2024-05-05T08:56:31.587993Z","shell.execute_reply.started":"2024-05-05T08:56:31.580077Z","shell.execute_reply":"2024-05-05T08:56:31.586921Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"inference(model, val_dl)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T09:01:48.499173Z","iopub.execute_input":"2024-05-05T09:01:48.499548Z","iopub.status.idle":"2024-05-05T09:02:26.055663Z","shell.execute_reply.started":"2024-05-05T09:01:48.499520Z","shell.execute_reply":"2024-05-05T09:02:26.054687Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Accuracy: 0.80, Total items: 1781\n","output_type":"stream"}]},{"cell_type":"code","source":"# Run inference on trained model with the validation set load best model weights\nmodel_inf = nn.DataParallel(AudioClassifier())\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_inf = model_inf.to(device)\ntry:\n    model_inf.load_state_dict(torch.load('/kaggle/working/model.pt'))\nexcept:\n    print('E')\nmodel_inf.eval()\ninference(model_inf, val_dl)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T09:01:02.268960Z","iopub.execute_input":"2024-05-05T09:01:02.269881Z","iopub.status.idle":"2024-05-05T09:01:17.294633Z","shell.execute_reply.started":"2024-05-05T09:01:02.269847Z","shell.execute_reply":"2024-05-05T09:01:17.293189Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Er\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[47], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m model_inf\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 10\u001b[0m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[32], line 10\u001b[0m, in \u001b[0;36minference\u001b[0;34m(model, test_dl)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Disable gradient updates\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m test_dl:\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# Get the input features and target labels, and put them on the GPU\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# Normalize the inputs\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n","Cell \u001b[0;32mIn[8], line 29\u001b[0m, in \u001b[0;36mSoundDS.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Get the Class ID\u001b[39;00m\n\u001b[1;32m     27\u001b[0m class_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mloc[idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassID\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 29\u001b[0m aud \u001b[38;5;241m=\u001b[39m \u001b[43mAudioUtil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Some sounds have a higher sample rate, or fewer channels compared to the\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# majority. So make all sounds have the same number of channels and same \u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# sample rate. Unless the sample rate is the same, the pad_trunc will still\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# result in arrays of different lengths, even though the sound duration is\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# the same.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m reaud \u001b[38;5;241m=\u001b[39m AudioUtil\u001b[38;5;241m.\u001b[39mresample(aud, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msr)\n","Cell \u001b[0;32mIn[6], line 7\u001b[0m, in \u001b[0;36mAudioUtil.open\u001b[0;34m(audio_file)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(audio_file):\n\u001b[0;32m----> 7\u001b[0m     sig, sr \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (sig, sr)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchaudio/_backend/utils.py:204\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m backend \u001b[38;5;241m=\u001b[39m dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchaudio/_backend/ffmpeg.py:336\u001b[0m, in \u001b[0;36mFFmpegBackend.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_audio_fileobj(\n\u001b[1;32m    327\u001b[0m         uri,\n\u001b[1;32m    328\u001b[0m         frame_offset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    333\u001b[0m         buffer_size,\n\u001b[1;32m    334\u001b[0m     )\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchaudio/_backend/ffmpeg.py:100\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_audio\u001b[39m(\n\u001b[1;32m     92\u001b[0m     src: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     93\u001b[0m     frame_offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mformat\u001b[39m: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     98\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;241m=\u001b[39m _get_load_filter(frame_offset, num_frames, convert)\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_ops.py:692\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import kaggle","metadata":{"execution":{"iopub.status.busy":"2024-05-05T09:06:42.514031Z","iopub.execute_input":"2024-05-05T09:06:42.514933Z","iopub.status.idle":"2024-05-05T09:06:43.026653Z","shell.execute_reply.started":"2024-05-05T09:06:42.514897Z","shell.execute_reply":"2024-05-05T09:06:43.025430Z"},"trusted":true},"execution_count":49,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkaggle\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kaggle/__init__.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkaggle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ApiClient\n\u001b[1;32m      6\u001b[0m api \u001b[38;5;241m=\u001b[39m KaggleApi(ApiClient())\n\u001b[0;32m----> 7\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthenticate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kaggle/api/kaggle_api_extended.py:398\u001b[0m, in \u001b[0;36mKaggleApi.authenticate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Make sure it\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms located in\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    399\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Or use the environment method.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    400\u001b[0m                           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_file, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_dir))\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# Step 3: load into configuration!\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_config(config_data)\n","\u001b[0;31mOSError\u001b[0m: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method."],"ename":"OSError","evalue":"Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}