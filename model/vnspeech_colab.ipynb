<<<<<<< HEAD
{"cells":[{"cell_type":"markdown","metadata":{"id":"imSHozTgnIXg"},"source":["# Link to drive and kaggle datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22220,"status":"ok","timestamp":1714904348117,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"c19KsPk_ItqM","outputId":"5b3efdc3-cfa3-4526-a5d2-59002980e67e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bY_iZqBNKTDw"},"outputs":[],"source":["!pip install -q kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"executionInfo":{"elapsed":19186,"status":"ok","timestamp":1714959592763,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"AU_I1GolKedG","outputId":"39553392-eeb6-4898-a2e4-0b75c75af517"},"outputs":[{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-e1e57a46-538e-4430-b02e-282ca1aeea70\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-e1e57a46-538e-4430-b02e-282ca1aeea70\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Saving kaggle.json to kaggle.json\n"]},{"data":{"text/plain":["{'kaggle.json': b'{\"username\":\"trnngci\",\"key\":\"c508f9166a87995edc8dab20a97f4fc4\"}'}"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["from google.colab import files\n","files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":370,"status":"ok","timestamp":1714959637230,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"FNcTGII3L1Hy","outputId":"54356815-8969-40d4-ec15-5b2e8dde34bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory â€˜/root/.kaggleâ€™: File exists\n"]}],"source":["!mkdir ~/.kaggle\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6Sw0veUMKD1"},"outputs":[],"source":["!cp /content/kaggle.json ~/.kaggle/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"plZuLb5EMTF_"},"outputs":[],"source":["!chmod 600 /.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TypOqeSUj-LF"},"outputs":[],"source":["!cd ~/.kaggle\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":977,"status":"ok","timestamp":1714959766552,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"vYhRCVVtMhOL","outputId":"5043582f-e0bc-49d4-bd4c-981ca9c43177"},"outputs":[{"name":"stdout","output_type":"stream","text":["Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n","ref                                                             title                                              size  lastUpdated          downloadCount  voteCount  usabilityRating  \n","--------------------------------------------------------------  ------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n","rahulvyasm/netflix-movies-and-tv-shows                          Netflix Movies and TV Shows                         1MB  2024-04-10 09:48:38          12506        264  1.0              \n","sahirmaharajj/school-student-daily-attendance                   School Student Daily Attendance                     2MB  2024-04-29 19:29:56           1449         33  1.0              \n","jaidalmotra/pokemon-dataset                                     Pokemon Dataset                                    19KB  2024-04-30 10:38:36           1039         35  1.0              \n","anandshaw2001/airlines-booking-csv                              Airlines_Booking.csv                              414KB  2024-04-20 17:38:50           2192         33  1.0              \n","fahadrehman07/retail-transaction-dataset                        Retail Transaction Dataset                          5MB  2024-05-01 10:05:25           1241         35  1.0              \n","adityakishor1/vehicle-sales-count-by-year-2002-2023             Vehicle_Sales_Count by Year 2002-2023               5KB  2024-04-20 09:33:07           1171         29  1.0              \n","mexwell/heart-disease-dataset                                   ðŸ«€ Heart Disease Dataset                           399KB  2024-04-08 09:43:49           5683         93  1.0              \n","aadarshvelu/aids-virus-infection-prediction                     AIDS Virus Infection Prediction ðŸ’‰                   2MB  2024-04-28 03:22:18           1221         37  1.0              \n","rabieelkharoua/predict-survival-of-patients-with-heart-failure  Predict survival of patients with heart failure     4KB  2024-04-25 10:21:47           1985         39  1.0              \n","jaceprater/smokers-health-data                                  Smoker's Health Data                               29KB  2024-04-20 17:09:38           2691         27  1.0              \n","varunraskar/store-sales-data                                    Store Sales Data                                   40KB  2024-04-05 08:46:03           1421         22  0.8235294        \n","sahirmaharajj/civil-service-exam-results                        Civil Service Exam Results                         12MB  2024-04-29 19:40:20            364         22  1.0              \n","sahirmaharajj/unemployment-by-age-groups-dataset                Unemployment by Age Groups Dataset                  3KB  2024-04-20 21:31:03           1697         39  1.0              \n","brianblakely/top-100-songs-and-lyrics-from-1959-to-2019         Top 100 Songs & Lyrics By Year 1959 - 2023 (USA)   24MB  2024-04-28 18:59:05            573         25  1.0              \n","sujithmandala/second-hand-car-price-prediction                  Second Hand Car Price Prediction                    2KB  2024-04-24 12:09:30           1961         34  1.0              \n","jacopoferretti/child-vs-mother-iq                               Child vs Mother IQ                                  8KB  2024-04-28 22:29:48            761         25  1.0              \n","chopper53/machine-learning-engineer-salary-in-2024              Machine Learning Engineer Salary in 2024          107KB  2024-04-23 17:30:13           1729         42  1.0              \n","bhavyadhingra00020/top-anime-dataset-2024                       Top Anime Dataset 2024                            459KB  2024-04-29 12:33:21            833         29  1.0              \n","priyamchoksi/bitcoin-historical-prices-and-activity-2010-2024   Bitcoin Historical Prices & Activity (2010-2024)  187KB  2024-04-26 13:01:20            735         27  1.0              \n","prishasawhney/mushroom-dataset                                  Mushroom Dataset (Binary Classification)          602KB  2024-04-18 19:56:44           2533         73  1.0              \n"]}],"source":["!kaggle datasets list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72521,"status":"ok","timestamp":1714959843286,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"qucK-QOFNA0Y","outputId":"568beaf2-8a37-43e0-d5ef-42d3912bda07"},"outputs":[{"name":"stdout","output_type":"stream","text":["Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n","Dataset URL: https://www.kaggle.com/datasets/trnngci/vietnamese-speech-labeled-by-region\n","License(s): Apache 2.0\n","Downloading vietnamese-speech-labeled-by-region.zip to /content\n","100% 5.35G/5.37G [01:10<00:00, 102MB/s]\n","100% 5.37G/5.37G [01:10<00:00, 81.3MB/s]\n"]}],"source":["! kaggle datasets download trnngci/vietnamese-speech-labeled-by-region\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btNgYNY-SF-C"},"outputs":[],"source":["! unzip vietnamese-speech-labeled-by-region.zip"]},{"cell_type":"markdown","metadata":{"id":"_OXgjLPmnOyX"},"source":["## Prepare training data from Metadata file"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":1062,"status":"ok","timestamp":1714960154200,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"fC3SZnmETW4W"},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","from pathlib import Path"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":525,"status":"ok","timestamp":1714960158970,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"Poy1zULZU39Q","outputId":"61164092-761b-476c-927d-4850aa818fcd"},"outputs":[{"name":"stdout","output_type":"stream","text":["       filename  region_fold  subregion_fold  classID  class_name  \\\n","0  06010001.wav            6             601        6  South West   \n","1  06010002.wav            6             601        6  South West   \n","2  06010003.wav            6             601        6  South West   \n","3  06010004.wav            6             601        6  South West   \n","4  06010005.wav            6             601        6  South West   \n","\n","           relative_path  \n","0  /06/0601/06010001.wav  \n","1  /06/0601/06010002.wav  \n","2  /06/0601/06010003.wav  \n","3  /06/0601/06010004.wav  \n","4  /06/0601/06010005.wav  \n"]}],"source":["# ----------------------------\n","# Prepare training data from Metadata file\n","# ----------------------------\n","\n","data_path = '/content/dataset/dataset'\n","\n","# Read metadata file\n","metadata_file = '/content/VNspeech.csv'\n","\n","df = pd.read_csv(metadata_file)\n","\n","df['relative_path'] = '/0' + df['region_fold'].astype(str) + '/0' + df['subregion_fold'].astype(str) + '/' + df['filename'].astype(str)\n","\n","# Take relevant columns\n","# df = df[['relative_path', 'classID']]\n","print(df.head())"]},{"cell_type":"markdown","metadata":{"id":"KW-x-T2enWZO"},"source":["## Load .wav data and processing data"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":504,"status":"ok","timestamp":1714960164344,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"2u3THLPKVPkz"},"outputs":[],"source":["import math, random\n","import torch\n","import torchaudio\n","from torchaudio import transforms\n","from IPython.display import Audio"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":337,"status":"ok","timestamp":1714960122285,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"8YxFmMH1VTM6"},"outputs":[],"source":["class AudioUtil():\n","   # ----------------------------\n","   # Load an audio file. Return the signal as a tensor and the sample rate\n","   # ----------------------------\n","    @staticmethod\n","    def open(audio_file):\n","        sig, sr = torchaudio.load(audio_file)\n","        return (sig, sr)\n","\n","\n","    # ----------------------------\n","    # Convert to two channels\n","    # ----------------------------\n","    # Convert the given audio to the desired number of channels\n","    # ----------------------------\n","    @staticmethod\n","    def rechannel(aud, new_channel):\n","        sig, sr = aud\n","\n","        if (sig.shape[0] == new_channel):\n","          # Nothing to do\n","            return aud\n","\n","        if (new_channel == 1):\n","          # Convert from stereo to mono by selecting only the first channel\n","            resig = sig[:1, :]\n","        else:\n","          # Convert from mono to stereo by duplicating the first channel\n","            resig = torch.cat([sig, sig])\n","\n","        return ((resig, sr))\n","\n","\n","    # ----------------------------\n","    # Standardize sampling rate\n","    # ----------------------------\n","    # Since Resample applies to a single channel, we resample one channel at a time\n","    # ----------------------------\n","    @staticmethod\n","    def resample(aud, newsr):\n","        sig, sr = aud\n","\n","        if (sr == newsr):\n","            # Nothing to do\n","            return aud\n","\n","        num_channels = sig.shape[0]\n","        # Resample first channel\n","        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n","        if (num_channels > 1):\n","            # Resample the second channel and merge both channels\n","            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n","            resig = torch.cat([resig, retwo])\n","\n","        return ((resig, newsr))\n","\n","\n","    # ----------------------------\n","    # Resize to the same length\n","    # ----------------------------\n","    # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n","    # ----------------------------\n","    @staticmethod\n","    def pad_trunc(aud, max_ms):\n","        sig, sr = aud\n","        num_rows    , sig_len = sig.shape\n","        max_len = sr//1000 * max_ms\n","\n","        if (sig_len > max_len):\n","            # Truncate the signal to the given length\n","            sig = sig[:,:max_len]\n","\n","        elif (sig_len < max_len):\n","            # Length of padding to add at the beginning and end of the signal\n","            pad_begin_len = random.randint(0, max_len - sig_len)\n","            pad_end_len = max_len - sig_len - pad_begin_len\n","\n","            # Pad with 0s\n","            pad_begin = torch.zeros((num_rows, pad_begin_len))\n","            pad_end = torch.zeros((num_rows, pad_end_len))\n","\n","            sig = torch.cat((pad_begin, sig, pad_end), 1)\n","\n","        return (sig, sr)\n","\n","    # ----------------------------\n","    # Data Augmentation: Time Shift\n","    # ----------------------------\n","    # Shifts the signal to the left or right by some percent. Values at the end\n","    # are 'wrapped around' to the start of the transformed signal.\n","    # ----------------------------\n","    @staticmethod\n","    def time_shift(aud, shift_limit):\n","        sig,sr = aud\n","        _, sig_len = sig.shape\n","        shift_amt = int(random.random() * shift_limit * sig_len)\n","        return (sig.roll(shift_amt), sr)\n","\n","\n","    # ----------------------------\n","    # Convert to Mel Spectrogram\n","    # ----------------------------\n","    # Generate a Spectrogram\n","    # ----------------------------\n","    @staticmethod\n","    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n","        sig,sr = aud\n","        top_db = 80\n","        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n","        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n","        # Convert to decibels\n","        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n","        return (spec)\n","\n","    # ----------------------------\n","    # Data Augmentation: Time and Frequency Masking\n","    # ----------------------------\n","    # Augment the Spectrogram by masking out some sections of it in both the frequency\n","    # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n","    # overfitting and to help the model generalise better. The masked sections are\n","    # replaced with the mean value.\n","    # ----------------------------\n","    @staticmethod\n","    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n","        _, n_mels, n_steps = spec.shape\n","        mask_value = spec.mean()\n","        aug_spec = spec\n","        freq_mask_param = max_mask_pct * n_mels\n","        for _ in range(n_freq_masks):\n","            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n","        time_mask_param = max_mask_pct * n_steps\n","        for _ in range(n_time_masks):\n","            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n","        return aug_spec"]},{"cell_type":"markdown","metadata":{"id":"53dttb-ond1Y"},"source":["## Dataset and DataLoader"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":332,"status":"ok","timestamp":1714960124440,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"MAK39_8BVlIh"},"outputs":[],"source":["from torch.utils.data import DataLoader, Dataset, random_split\n","import torchaudio"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1714960124815,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"BC_8igLYVsff"},"outputs":[],"source":["# ----------------------------\n","# Sound Dataset\n","# ----------------------------\n","class SoundDS(Dataset):\n","    def __init__(self, df, data_path):\n","        self.df = df\n","        self.data_path = str(data_path)\n","        self.duration = 8000\n","        self.sr = 44100\n","        self.channel = 2\n","        self.shift_pct = 0.4\n","\n","    # ----------------------------\n","    # Number of items in dataset\n","    # ----------------------------\n","    def __len__(self):\n","        return len(self.df)\n","\n","    # ----------------------------\n","    # Get i'th item in dataset\n","    # ----------------------------\n","    def __getitem__(self, idx):\n","        # Absolute file path of the audio file - concatenate the audio directory with\n","        # the relative path\n","        audio_file = self.data_path + self.df.loc[idx, 'relative_path']\n","        # Get the Class ID\n","        class_id = self.df.loc[idx, 'classID']\n","\n","        aud = AudioUtil.open(audio_file)\n","        # Some sounds have a higher sample rate, or fewer channels compared to the\n","        # majority. So make all sounds have the same number of channels and same\n","        # sample rate. Unless the sample rate is the same, the pad_trunc will still\n","        # result in arrays of different lengths, even though the sound duration is\n","        # the same.\n","        reaud = AudioUtil.resample(aud, self.sr)\n","        rechan = AudioUtil.rechannel(reaud, self.channel)\n","\n","        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n","        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n","        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n","        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n","\n","        return aug_sgram, class_id"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1714960125263,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"K_E2NqC9VvSO"},"outputs":[],"source":["    from torch.utils.data import random_split\n"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":494,"status":"ok","timestamp":1714960170310,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"_c7RuwDPVxTv"},"outputs":[],"source":["myds = SoundDS(df, data_path)\n","\n","# Random split of 80:20 between training and validation\n","num_items = len(myds)\n","num_train = round(num_items * 0.8)\n","num_val = num_items - num_train\n","train_ds, val_ds = random_split(myds, [num_train, num_val])\n","\n","# Create training and validation data loaders\n","train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n","val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"Ej0feS24nqaN"},"source":["# Model"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":337,"status":"ok","timestamp":1714960138176,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"9Vrd5yArVz-a"},"outputs":[],"source":["import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.nn import init"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":503,"status":"ok","timestamp":1714960176394,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"u3F9pOYVWFfV"},"outputs":[],"source":["# ----------------------------\n","# Audio Classification Model\n","# ----------------------------\n","class AudioClassifier (nn.Module):\n","    # ----------------------------\n","    # Build the model architecture\n","    # ----------------------------\n","    def __init__(self):\n","        super().__init__()\n","        conv_layers = []\n","\n","        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n","        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n","        self.relu1 = nn.ReLU()\n","        self.bn1 = nn.BatchNorm2d(8)\n","        init.kaiming_normal_(self.conv1.weight, a=0.1)\n","        self.conv1.bias.data.zero_()\n","        conv_layers += [self.conv1, self.relu1, self.bn1]\n","\n","        # Second Convolution Block\n","        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu2 = nn.ReLU()\n","        self.bn2 = nn.BatchNorm2d(16)\n","        init.kaiming_normal_(self.conv2.weight, a=0.1)\n","        self.conv2.bias.data.zero_()\n","        conv_layers += [self.conv2, self.relu2, self.bn2]\n","\n","        # Second Convolution Block\n","        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu3 = nn.ReLU()\n","        self.bn3 = nn.BatchNorm2d(32)\n","        init.kaiming_normal_(self.conv3.weight, a=0.1)\n","        self.conv3.bias.data.zero_()\n","        conv_layers += [self.conv3, self.relu3, self.bn3]\n","\n","        # Second Convolution Block\n","        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu4 = nn.ReLU()\n","        self.bn4 = nn.BatchNorm2d(64)\n","        init.kaiming_normal_(self.conv4.weight, a=0.1)\n","        self.conv4.bias.data.zero_()\n","        conv_layers += [self.conv4, self.relu4, self.bn4]\n","\n","        # Linear Classifier\n","        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n","        self.lin = nn.Linear(in_features=64, out_features=10)\n","\n","        # Wrap the Convolutional Blocks\n","        self.conv = nn.Sequential(*conv_layers)\n","\n","    # ----------------------------\n","    # Forward pass computations\n","    # ----------------------------\n","    def forward(self, x):\n","        # Run the convolutional blocks\n","        x = self.conv(x)\n","\n","        # Adaptive pool and flatten for input to linear layer\n","        x = self.ap(x)\n","        x = x.view(x.shape[0], -1)\n","\n","        # Linear layer\n","        x = self.lin(x)\n","\n","        # Final output\n","        return x\n","\n","# Create the model\n","model = AudioClassifier()"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":551,"status":"ok","timestamp":1714960178542,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"NS6RE1ESWUGk","outputId":"f7273371-0101-48b4-87da-23d2943fafc3"},"outputs":[{"data":{"text/plain":["device(type='cuda', index=0)"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["os.environ['TORCH_USE_CUDA_DSA'] = '1'\n","# Move the model to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","# Check that it is on Cuda\n","next(model.parameters()).device"]},{"cell_type":"markdown","metadata":{"id":"L17dKLHGnvcb"},"source":["# Trainning"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":498,"status":"ok","timestamp":1714960221656,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"1hpiIefcWUtk"},"outputs":[],"source":["# ----------------------------\n","# Training Loop\n","# ----------------------------\n","def training(model, train_dl, num_epochs):\n","    # Loss Function, Optimizer and Scheduler\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n","    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n","                                                steps_per_epoch=int(len(train_dl)),\n","                                                epochs=num_epochs,\n","                                                anneal_strategy='linear')\n","\n","    # Repeat for each epoch\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        correct_prediction = 0\n","        total_prediction = 0\n","\n","        # Repeat for each batch in the training set\n","        for i, data in enumerate(train_dl):\n","            # Get the input features and target labels, and put them on the GPU\n","            inputs, labels = data[0].to(device), data[1].to(device)\n","\n","            # Normalize the inputs\n","            inputs_m, inputs_s = inputs.mean(), inputs.std()\n","            inputs = (inputs - inputs_m) / inputs_s\n","\n","            model.train()\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Keep stats for Loss and Accuracy\n","            running_loss += loss.item()\n","\n","            # Get the predicted class with the highest score\n","            _, prediction = torch.max(outputs,1)\n","            # Count of predictions that matched the target label\n","            correct_prediction += (prediction == labels).sum().item()\n","            total_prediction += prediction.shape[0]\n","            #if i % 10 == 0:    # print every 10 mini-batches\n","            #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n","\n","        # Print stats at the end of the epoch\n","        num_batches = len(train_dl)\n","        avg_loss = running_loss / num_batches\n","        avg_acc = correct_prediction/total_prediction\n","        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {avg_acc:.2f}')\n","\n","    print('Finished Training')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2OSliLOdmGYp"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oaiHYbqBWju3"},"outputs":[],"source":["num_epochs=20\n","training(model, train_dl, num_epochs)\n","\n","try:\n","    torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/vnspeech/model1.pt')\n","except:\n","    print(\"r\")"]},{"cell_type":"markdown","metadata":{"id":"zFvX-pFZnyoE"},"source":["# inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mP3RBFmaW2Pz"},"outputs":[],"source":["def inference (model, test_dl):\n","    correct_prediction = 0\n","    total_prediction = 0\n","\n","    # Disable gradient updates\n","    with torch.no_grad():\n","        for data in test_dl:\n","            # Get the input features and target labels, and put them on the GPU\n","            inputs, labels = data[0].to(device), data[1].to(device)\n","\n","            # Normalize the inputs\n","            inputs_m, inputs_s = inputs.mean(), inputs.std()\n","            inputs = (inputs - inputs_m) / inputs_s\n","\n","            # Get predictions\n","            outputs = model(inputs)\n","\n","            # Get the predicted class with the highest score\n","            _, prediction = torch.max(outputs,1)\n","            # Count of predictions that matched the target label\n","            correct_prediction += (prediction == labels).sum().item()\n","            total_prediction += prediction.shape[0]\n","\n","    acc = correct_prediction/total_prediction\n","    print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rYpKzBJGhDZa"},"outputs":[],"source":["# Specify the path to your .pt file\n","file_path = '/content/drive/MyDrive/Colab Notebooks/vnspeech/model1.pt'\n","\n","# Load the model checkpoint\n","checkpoint = torch.load(file_path)\n","\n","print('checkpoint.keys = ', checkpoint.keys())"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":336,"status":"ok","timestamp":1714960244063,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"jM8l4azshGDh","outputId":"4a740e71-10d5-4b39-f044-ea25e32480ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["checkpoint.keys =  odict_keys(['module.conv1.weight', 'module.conv1.bias', 'module.bn1.weight', 'module.bn1.bias', 'module.bn1.running_mean', 'module.bn1.running_var', 'module.bn1.num_batches_tracked', 'module.conv2.weight', 'module.conv2.bias', 'module.bn2.weight', 'module.bn2.bias', 'module.bn2.running_mean', 'module.bn2.running_var', 'module.bn2.num_batches_tracked', 'module.conv3.weight', 'module.conv3.bias', 'module.bn3.weight', 'module.bn3.bias', 'module.bn3.running_mean', 'module.bn3.running_var', 'module.bn3.num_batches_tracked', 'module.conv4.weight', 'module.conv4.bias', 'module.bn4.weight', 'module.bn4.bias', 'module.bn4.running_mean', 'module.bn4.running_var', 'module.bn4.num_batches_tracked', 'module.lin.weight', 'module.lin.bias', 'module.conv.0.weight', 'module.conv.0.bias', 'module.conv.2.weight', 'module.conv.2.bias', 'module.conv.2.running_mean', 'module.conv.2.running_var', 'module.conv.2.num_batches_tracked', 'module.conv.3.weight', 'module.conv.3.bias', 'module.conv.5.weight', 'module.conv.5.bias', 'module.conv.5.running_mean', 'module.conv.5.running_var', 'module.conv.5.num_batches_tracked', 'module.conv.6.weight', 'module.conv.6.bias', 'module.conv.8.weight', 'module.conv.8.bias', 'module.conv.8.running_mean', 'module.conv.8.running_var', 'module.conv.8.num_batches_tracked', 'module.conv.9.weight', 'module.conv.9.bias', 'module.conv.11.weight', 'module.conv.11.bias', 'module.conv.11.running_mean', 'module.conv.11.running_var', 'module.conv.11.num_batches_tracked'])\n"]}],"source":["for key in list(checkpoint.keys()):\n","  if not ('module.' in key):\n","    checkpoint['module.' + key] = checkpoint[key]\n","    del checkpoint[key]\n","\n","print('checkpoint.keys = ', checkpoint.keys())"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":325,"status":"ok","timestamp":1714960472770,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"AFjea0b5hIQB","outputId":"5b20c7c9-d0cb-444c-f075-6d798e9016f7"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["model_inf = nn.DataParallel(AudioClassifier())\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_inf = model_inf.to(device)\n","model_inf.load_state_dict(checkpoint)\n"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3594848,"status":"ok","timestamp":1714964106388,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"rQW2etb1nJOH","outputId":"83e83be3-0df4-42de-de69-6871620b9e44"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0, Loss: 0.47, Accuracy: 0.81\n","Epoch: 1, Loss: 0.48, Accuracy: 0.81\n","Epoch: 2, Loss: 0.47, Accuracy: 0.81\n","Epoch: 3, Loss: 0.47, Accuracy: 0.81\n","Epoch: 4, Loss: 0.49, Accuracy: 0.81\n","Epoch: 5, Loss: 0.47, Accuracy: 0.81\n","Epoch: 6, Loss: 0.47, Accuracy: 0.81\n","Epoch: 7, Loss: 0.45, Accuracy: 0.82\n","Epoch: 8, Loss: 0.43, Accuracy: 0.83\n","Epoch: 9, Loss: 0.43, Accuracy: 0.82\n","Epoch: 10, Loss: 0.42, Accuracy: 0.83\n","Epoch: 11, Loss: 0.42, Accuracy: 0.83\n","Epoch: 12, Loss: 0.41, Accuracy: 0.83\n","Epoch: 13, Loss: 0.39, Accuracy: 0.84\n","Epoch: 14, Loss: 0.39, Accuracy: 0.84\n","Epoch: 15, Loss: 0.39, Accuracy: 0.84\n","Epoch: 16, Loss: 0.37, Accuracy: 0.85\n","Epoch: 17, Loss: 0.37, Accuracy: 0.86\n","Epoch: 18, Loss: 0.36, Accuracy: 0.86\n","Epoch: 19, Loss: 0.37, Accuracy: 0.86\n","Finished Training\n"]}],"source":["num_epochs=20\n","training(model_inf, train_dl, num_epochs)\n","\n","try:\n","    torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/vnspeech/model1.pt')\n","except:\n","    print(\"r\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46065,"status":"ok","timestamp":1714909915330,"user":{"displayName":"Ngoc Dai Tran","userId":"01359618113952775150"},"user_tz":-420},"id":"PfTRIKsUlpvg","outputId":"d7d9a917-dc27-4ec4-8de7-118e23621593"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.83, Total items: 1781\n"]}],"source":["inference(model_inf, val_dl)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hO-gPDjyiROX"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOo41gc6mSOdZp+mzHq/WJl","gpuType":"T4","mount_file_id":"14iYRe3bXNEfXD4_1CHdG4KUnFTwB2UDo","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
=======
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Link to drive and kaggle datasets"
      ],
      "metadata": {
        "id": "imSHozTgnIXg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c19KsPk_ItqM",
        "outputId": "5b3efdc3-cfa3-4526-a5d2-59002980e67e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "bY_iZqBNKTDw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "AU_I1GolKedG",
        "outputId": "a2a40c01-1d65-419e-a072-6744d9d6ce1a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8ff48290-9e6b-41f8-8e78-0cc9d6007e03\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8ff48290-9e6b-41f8-8e78-0cc9d6007e03\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"trnngci\",\"key\":\"c508f9166a87995edc8dab20a97f4fc4\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n"
      ],
      "metadata": {
        "id": "FNcTGII3L1Hy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "f6Sw0veUMKD1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 /.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "plZuLb5EMTF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYhRCVVtMhOL",
        "outputId": "e9c8fdbf-56f6-45a7-b29c-e8b907b0a90d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "ref                                                             title                                              size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------------------  ------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "rahulvyasm/netflix-movies-and-tv-shows                          Netflix Movies and TV Shows                         1MB  2024-04-10 09:48:38          12114        254  1.0              \n",
            "adityakishor1/vehicle-sales-count-by-year-2002-2023             Vehicle_Sales_Count by Year 2002-2023               5KB  2024-04-20 09:33:07           1112         27  1.0              \n",
            "anandshaw2001/airlines-booking-csv                              Airlines_Booking.csv                              414KB  2024-04-20 17:38:50           2110         33  1.0              \n",
            "jaidalmotra/pokemon-dataset                                     Pokemon Dataset                                    19KB  2024-04-30 10:38:36            990         35  1.0              \n",
            "fahadrehman07/retail-transaction-dataset                        Retail Transaction Dataset                          5MB  2024-05-01 10:05:25           1177         35  1.0              \n",
            "mexwell/heart-disease-dataset                                   ðŸ«€ Heart Disease Dataset                           399KB  2024-04-08 09:43:49           5523         92  1.0              \n",
            "aadarshvelu/aids-virus-infection-prediction                     AIDS Virus Infection Prediction ðŸ’‰                   2MB  2024-04-28 03:22:18           1130         36  1.0              \n",
            "rabieelkharoua/predict-survival-of-patients-with-heart-failure  Predict survival of patients with heart failure     4KB  2024-04-25 10:21:47           1915         39  1.0              \n",
            "sahirmaharajj/school-student-daily-attendance                   School Student Daily Attendance                     2MB  2024-04-29 19:29:56           1327         27  1.0              \n",
            "jaceprater/smokers-health-data                                  Smoker's Health Data                               29KB  2024-04-20 17:09:38           2624         27  1.0              \n",
            "sujithmandala/second-hand-car-price-prediction                  Second Hand Car Price Prediction                    2KB  2024-04-24 12:09:30           1895         34  1.0              \n",
            "chopper53/machine-learning-engineer-salary-in-2024              Machine Learning Engineer Salary in 2024          107KB  2024-04-23 17:30:13           1654         42  1.0              \n",
            "varunraskar/store-sales-data                                    Store Sales Data                                   40KB  2024-04-05 08:46:03           1373         22  0.8235294        \n",
            "sahirmaharajj/civil-service-exam-results                        Civil Service Exam Results                         12MB  2024-04-29 19:40:20            324         22  1.0              \n",
            "sahirmaharajj/unemployment-by-age-groups-dataset                Unemployment by Age Groups Dataset                  3KB  2024-04-20 21:31:03           1648         38  1.0              \n",
            "prishasawhney/mushroom-dataset                                  Mushroom Dataset (Binary Classification)          602KB  2024-04-18 19:56:44           2464         73  1.0              \n",
            "brianblakely/top-100-songs-and-lyrics-from-1959-to-2019         Top 100 Songs & Lyrics By Year 1959 - 2023 (USA)   24MB  2024-04-28 18:59:05            537         24  1.0              \n",
            "jacopoferretti/child-vs-mother-iq                               Child vs Mother IQ                                  8KB  2024-04-28 22:29:48            732         25  1.0              \n",
            "bhavyadhingra00020/top-anime-dataset-2024                       Top Anime Dataset 2024                            459KB  2024-04-29 12:33:21            769         29  1.0              \n",
            "priyamchoksi/bitcoin-historical-prices-and-activity-2010-2024   Bitcoin Historical Prices & Activity (2010-2024)  187KB  2024-04-26 13:01:20            703         27  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download trnngci/vietnamese-speech-labeled-by-region\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qucK-QOFNA0Y",
        "outputId": "c9d67001-8e4c-40bb-c557-eaa277d683f2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/trnngci/vietnamese-speech-labeled-by-region\n",
            "License(s): Apache 2.0\n",
            "Downloading vietnamese-speech-labeled-by-region.zip to /content\n",
            "100% 5.37G/5.37G [05:01<00:00, 14.7MB/s]\n",
            "100% 5.37G/5.37G [05:01<00:00, 19.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip vietnamese-speech-labeled-by-region.zip"
      ],
      "metadata": {
        "id": "btNgYNY-SF-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare training data from Metadata file"
      ],
      "metadata": {
        "id": "_OXgjLPmnOyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "fC3SZnmETW4W"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Prepare training data from Metadata file\n",
        "# ----------------------------\n",
        "\n",
        "data_path = '/content/dataset/dataset'\n",
        "\n",
        "# Read metadata file\n",
        "metadata_file = '/content/VNspeech.csv'\n",
        "\n",
        "df = pd.read_csv(metadata_file)\n",
        "\n",
        "df['relative_path'] = '/0' + df['region_fold'].astype(str) + '/0' + df['subregion_fold'].astype(str) + '/' + df['filename'].astype(str)\n",
        "\n",
        "# Take relevant columns\n",
        "# df = df[['relative_path', 'classID']]\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Poy1zULZU39Q",
        "outputId": "1e70e4c8-12cc-484b-f717-913665ef91b1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       filename  region_fold  subregion_fold  classID  class_name  \\\n",
            "0  06010001.wav            6             601        6  South West   \n",
            "1  06010002.wav            6             601        6  South West   \n",
            "2  06010003.wav            6             601        6  South West   \n",
            "3  06010004.wav            6             601        6  South West   \n",
            "4  06010005.wav            6             601        6  South West   \n",
            "\n",
            "           relative_path  \n",
            "0  /06/0601/06010001.wav  \n",
            "1  /06/0601/06010002.wav  \n",
            "2  /06/0601/06010003.wav  \n",
            "3  /06/0601/06010004.wav  \n",
            "4  /06/0601/06010005.wav  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load .wav data and processing data"
      ],
      "metadata": {
        "id": "KW-x-T2enWZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math, random\n",
        "import torch\n",
        "import torchaudio\n",
        "from torchaudio import transforms\n",
        "from IPython.display import Audio"
      ],
      "metadata": {
        "id": "2u3THLPKVPkz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioUtil():\n",
        "   # ----------------------------\n",
        "   # Load an audio file. Return the signal as a tensor and the sample rate\n",
        "   # ----------------------------\n",
        "    @staticmethod\n",
        "    def open(audio_file):\n",
        "        sig, sr = torchaudio.load(audio_file)\n",
        "        return (sig, sr)\n",
        "\n",
        "\n",
        "    # ----------------------------\n",
        "    # Convert to two channels\n",
        "    # ----------------------------\n",
        "    # Convert the given audio to the desired number of channels\n",
        "    # ----------------------------\n",
        "    @staticmethod\n",
        "    def rechannel(aud, new_channel):\n",
        "        sig, sr = aud\n",
        "\n",
        "        if (sig.shape[0] == new_channel):\n",
        "          # Nothing to do\n",
        "            return aud\n",
        "\n",
        "        if (new_channel == 1):\n",
        "          # Convert from stereo to mono by selecting only the first channel\n",
        "            resig = sig[:1, :]\n",
        "        else:\n",
        "          # Convert from mono to stereo by duplicating the first channel\n",
        "            resig = torch.cat([sig, sig])\n",
        "\n",
        "        return ((resig, sr))\n",
        "\n",
        "\n",
        "    # ----------------------------\n",
        "    # Standardize sampling rate\n",
        "    # ----------------------------\n",
        "    # Since Resample applies to a single channel, we resample one channel at a time\n",
        "    # ----------------------------\n",
        "    @staticmethod\n",
        "    def resample(aud, newsr):\n",
        "        sig, sr = aud\n",
        "\n",
        "        if (sr == newsr):\n",
        "            # Nothing to do\n",
        "            return aud\n",
        "\n",
        "        num_channels = sig.shape[0]\n",
        "        # Resample first channel\n",
        "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
        "        if (num_channels > 1):\n",
        "            # Resample the second channel and merge both channels\n",
        "            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
        "            resig = torch.cat([resig, retwo])\n",
        "\n",
        "        return ((resig, newsr))\n",
        "\n",
        "\n",
        "    # ----------------------------\n",
        "    # Resize to the same length\n",
        "    # ----------------------------\n",
        "    # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n",
        "    # ----------------------------\n",
        "    @staticmethod\n",
        "    def pad_trunc(aud, max_ms):\n",
        "        sig, sr = aud\n",
        "        num_rows, sig_len = sig.shape\n",
        "        max_len = sr//1000 * max_ms\n",
        "\n",
        "        if (sig_len > max_len):\n",
        "            # Truncate the signal to the given length\n",
        "            sig = sig[:,:max_len]\n",
        "\n",
        "        elif (sig_len < max_len):\n",
        "            # Length of padding to add at the beginning and end of the signal\n",
        "            pad_begin_len = random.randint(0, max_len - sig_len)\n",
        "            pad_end_len = max_len - sig_len - pad_begin_len\n",
        "\n",
        "            # Pad with 0s\n",
        "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
        "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
        "\n",
        "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
        "\n",
        "        return (sig, sr)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Data Augmentation: Time Shift\n",
        "    # ----------------------------\n",
        "    # Shifts the signal to the left or right by some percent. Values at the end\n",
        "    # are 'wrapped around' to the start of the transformed signal.\n",
        "    # ----------------------------\n",
        "    @staticmethod\n",
        "    def time_shift(aud, shift_limit):\n",
        "        sig,sr = aud\n",
        "        _, sig_len = sig.shape\n",
        "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
        "        return (sig.roll(shift_amt), sr)\n",
        "\n",
        "\n",
        "    # ----------------------------\n",
        "    # Convert to Mel Spectrogram\n",
        "    # ----------------------------\n",
        "    # Generate a Spectrogram\n",
        "    # ----------------------------\n",
        "    @staticmethod\n",
        "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
        "        sig,sr = aud\n",
        "        top_db = 80\n",
        "        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
        "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
        "        # Convert to decibels\n",
        "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
        "        return (spec)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Data Augmentation: Time and Frequency Masking\n",
        "    # ----------------------------\n",
        "    # Augment the Spectrogram by masking out some sections of it in both the frequency\n",
        "    # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n",
        "    # overfitting and to help the model generalise better. The masked sections are\n",
        "    # replaced with the mean value.\n",
        "    # ----------------------------\n",
        "    @staticmethod\n",
        "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
        "        _, n_mels, n_steps = spec.shape\n",
        "        mask_value = spec.mean()\n",
        "        aug_spec = spec\n",
        "        freq_mask_param = max_mask_pct * n_mels\n",
        "        for _ in range(n_freq_masks):\n",
        "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
        "        time_mask_param = max_mask_pct * n_steps\n",
        "        for _ in range(n_time_masks):\n",
        "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
        "        return aug_spec"
      ],
      "metadata": {
        "id": "8YxFmMH1VTM6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and DataLoader"
      ],
      "metadata": {
        "id": "53dttb-ond1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torchaudio"
      ],
      "metadata": {
        "id": "MAK39_8BVlIh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Sound Dataset\n",
        "# ----------------------------\n",
        "class SoundDS(Dataset):\n",
        "    def __init__(self, df, data_path):\n",
        "        self.df = df\n",
        "        self.data_path = str(data_path)\n",
        "        self.duration = 4000\n",
        "        self.sr = 44100\n",
        "        self.channel = 2\n",
        "        self.shift_pct = 0.4\n",
        "\n",
        "    # ----------------------------\n",
        "    # Number of items in dataset\n",
        "    # ----------------------------\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Get i'th item in dataset\n",
        "    # ----------------------------\n",
        "    def __getitem__(self, idx):\n",
        "        # Absolute file path of the audio file - concatenate the audio directory with\n",
        "        # the relative path\n",
        "        audio_file = self.data_path + self.df.loc[idx, 'relative_path']\n",
        "        # Get the Class ID\n",
        "        class_id = self.df.loc[idx, 'classID']\n",
        "\n",
        "        aud = AudioUtil.open(audio_file)\n",
        "        # Some sounds have a higher sample rate, or fewer channels compared to the\n",
        "        # majority. So make all sounds have the same number of channels and same\n",
        "        # sample rate. Unless the sample rate is the same, the pad_trunc will still\n",
        "        # result in arrays of different lengths, even though the sound duration is\n",
        "        # the same.\n",
        "        reaud = AudioUtil.resample(aud, self.sr)\n",
        "        rechan = AudioUtil.rechannel(reaud, self.channel)\n",
        "\n",
        "        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
        "        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
        "        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
        "        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
        "\n",
        "        return aug_sgram, class_id"
      ],
      "metadata": {
        "id": "BC_8igLYVsff"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n"
      ],
      "metadata": {
        "id": "K_E2NqC9VvSO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myds = SoundDS(df, data_path)\n",
        "\n",
        "# Random split of 80:20 between training and validation\n",
        "num_items = len(myds)\n",
        "num_train = round(num_items * 0.8)\n",
        "num_val = num_items - num_train\n",
        "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
        "\n",
        "# Create training and validation data loaders\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "_c7RuwDPVxTv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "Ej0feS24nqaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn import init"
      ],
      "metadata": {
        "id": "9Vrd5yArVz-a"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Audio Classification Model\n",
        "# ----------------------------\n",
        "class AudioClassifier (nn.Module):\n",
        "    # ----------------------------\n",
        "    # Build the model architecture\n",
        "    # ----------------------------\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        conv_layers = []\n",
        "\n",
        "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
        "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
        "        self.conv1.bias.data.zero_()\n",
        "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
        "\n",
        "        # Second Convolution Block\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
        "        self.conv2.bias.data.zero_()\n",
        "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
        "\n",
        "        # Second Convolution Block\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
        "        self.conv3.bias.data.zero_()\n",
        "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
        "\n",
        "        # Second Convolution Block\n",
        "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
        "        self.conv4.bias.data.zero_()\n",
        "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
        "\n",
        "        # Linear Classifier\n",
        "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.lin = nn.Linear(in_features=64, out_features=10)\n",
        "\n",
        "        # Wrap the Convolutional Blocks\n",
        "        self.conv = nn.Sequential(*conv_layers)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Forward pass computations\n",
        "    # ----------------------------\n",
        "    def forward(self, x):\n",
        "        # Run the convolutional blocks\n",
        "        x = self.conv(x)\n",
        "\n",
        "        # Adaptive pool and flatten for input to linear layer\n",
        "        x = self.ap(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Linear layer\n",
        "        x = self.lin(x)\n",
        "\n",
        "        # Final output\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "model = AudioClassifier()"
      ],
      "metadata": {
        "id": "u3F9pOYVWFfV"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "# Check that it is on Cuda\n",
        "next(model.parameters()).device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS6RE1ESWUGk",
        "outputId": "3f83103c-3eb1-4bd0-f4de-a8a0dd6d2264"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainning"
      ],
      "metadata": {
        "id": "L17dKLHGnvcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Training Loop\n",
        "# ----------------------------\n",
        "def training(model, train_dl, num_epochs):\n",
        "    # Loss Function, Optimizer and Scheduler\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
        "                                                steps_per_epoch=int(len(train_dl)),\n",
        "                                                epochs=num_epochs,\n",
        "                                                anneal_strategy='linear')\n",
        "\n",
        "    # Repeat for each epoch\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct_prediction = 0\n",
        "        total_prediction = 0\n",
        "\n",
        "        # Repeat for each batch in the training set\n",
        "        for i, data in enumerate(train_dl):\n",
        "            # Get the input features and target labels, and put them on the GPU\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # Normalize the inputs\n",
        "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
        "            inputs = (inputs - inputs_m) / inputs_s\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Keep stats for Loss and Accuracy\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Get the predicted class with the highest score\n",
        "            _, prediction = torch.max(outputs,1)\n",
        "            # Count of predictions that matched the target label\n",
        "            correct_prediction += (prediction == labels).sum().item()\n",
        "            total_prediction += prediction.shape[0]\n",
        "            #if i % 10 == 0:    # print every 10 mini-batches\n",
        "            #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
        "\n",
        "        # Print stats at the end of the epoch\n",
        "        num_batches = len(train_dl)\n",
        "        avg_loss = running_loss / num_batches\n",
        "        avg_acc = correct_prediction/total_prediction\n",
        "        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {avg_acc:.2f}')\n",
        "\n",
        "    print('Finished Training')"
      ],
      "metadata": {
        "id": "1hpiIefcWUtk"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=10\n",
        "training(model, train_dl, num_epochs)\n",
        "\n",
        "try:\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/vnspeech/model.pt')\n",
        "except:\n",
        "    print(\"r\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaiHYbqBWju3",
        "outputId": "09f70cef-9336-4a46-ec4c-474b7cf9d7a3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 1.66, Accuracy: 0.51\n",
            "Epoch: 1, Loss: 0.91, Accuracy: 0.66\n",
            "Epoch: 2, Loss: 0.71, Accuracy: 0.72\n",
            "Epoch: 3, Loss: 0.62, Accuracy: 0.75\n",
            "Epoch: 4, Loss: 0.59, Accuracy: 0.76\n",
            "Epoch: 5, Loss: 0.55, Accuracy: 0.78\n",
            "Epoch: 6, Loss: 0.53, Accuracy: 0.79\n",
            "Epoch: 7, Loss: 0.51, Accuracy: 0.80\n",
            "Epoch: 8, Loss: 0.48, Accuracy: 0.81\n",
            "Epoch: 9, Loss: 0.48, Accuracy: 0.81\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# inference"
      ],
      "metadata": {
        "id": "zFvX-pFZnyoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference (model, test_dl):\n",
        "    correct_prediction = 0\n",
        "    total_prediction = 0\n",
        "\n",
        "    # Disable gradient updates\n",
        "    with torch.no_grad():\n",
        "        for data in test_dl:\n",
        "            # Get the input features and target labels, and put them on the GPU\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # Normalize the inputs\n",
        "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
        "            inputs = (inputs - inputs_m) / inputs_s\n",
        "\n",
        "            # Get predictions\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Get the predicted class with the highest score\n",
        "            _, prediction = torch.max(outputs,1)\n",
        "            # Count of predictions that matched the target label\n",
        "            correct_prediction += (prediction == labels).sum().item()\n",
        "            total_prediction += prediction.shape[0]\n",
        "\n",
        "    acc = correct_prediction/total_prediction\n",
        "    print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')"
      ],
      "metadata": {
        "id": "mP3RBFmaW2Pz"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your .pt file\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/vnspeech/model.pt'\n",
        "\n",
        "# Load the model checkpoint\n",
        "checkpoint = torch.load(file_path)\n",
        "\n",
        "print('checkpoint.keys = ', checkpoint.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYpKzBJGhDZa",
        "outputId": "f5a01ea4-fb26-48e0-bbfe-90a8a2caf1c1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint.keys =  odict_keys(['conv1.weight', 'conv1.bias', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'conv2.weight', 'conv2.bias', 'bn2.weight', 'bn2.bias', 'bn2.running_mean', 'bn2.running_var', 'bn2.num_batches_tracked', 'conv3.weight', 'conv3.bias', 'bn3.weight', 'bn3.bias', 'bn3.running_mean', 'bn3.running_var', 'bn3.num_batches_tracked', 'conv4.weight', 'conv4.bias', 'bn4.weight', 'bn4.bias', 'bn4.running_mean', 'bn4.running_var', 'bn4.num_batches_tracked', 'lin.weight', 'lin.bias', 'conv.0.weight', 'conv.0.bias', 'conv.2.weight', 'conv.2.bias', 'conv.2.running_mean', 'conv.2.running_var', 'conv.2.num_batches_tracked', 'conv.3.weight', 'conv.3.bias', 'conv.5.weight', 'conv.5.bias', 'conv.5.running_mean', 'conv.5.running_var', 'conv.5.num_batches_tracked', 'conv.6.weight', 'conv.6.bias', 'conv.8.weight', 'conv.8.bias', 'conv.8.running_mean', 'conv.8.running_var', 'conv.8.num_batches_tracked', 'conv.9.weight', 'conv.9.bias', 'conv.11.weight', 'conv.11.bias', 'conv.11.running_mean', 'conv.11.running_var', 'conv.11.num_batches_tracked'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in list(checkpoint.keys()):\n",
        "  if not ('module.' in key):\n",
        "    checkpoint['module.' + key] = checkpoint[key]\n",
        "    del checkpoint[key]\n",
        "\n",
        "print('checkpoint.keys = ', checkpoint.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM8l4azshGDh",
        "outputId": "9c97aa33-b7cb-4f71-e907-5712491d6350"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint.keys =  odict_keys(['module.conv1.weight', 'module.conv1.bias', 'module.bn1.weight', 'module.bn1.bias', 'module.bn1.running_mean', 'module.bn1.running_var', 'module.bn1.num_batches_tracked', 'module.conv2.weight', 'module.conv2.bias', 'module.bn2.weight', 'module.bn2.bias', 'module.bn2.running_mean', 'module.bn2.running_var', 'module.bn2.num_batches_tracked', 'module.conv3.weight', 'module.conv3.bias', 'module.bn3.weight', 'module.bn3.bias', 'module.bn3.running_mean', 'module.bn3.running_var', 'module.bn3.num_batches_tracked', 'module.conv4.weight', 'module.conv4.bias', 'module.bn4.weight', 'module.bn4.bias', 'module.bn4.running_mean', 'module.bn4.running_var', 'module.bn4.num_batches_tracked', 'module.lin.weight', 'module.lin.bias', 'module.conv.0.weight', 'module.conv.0.bias', 'module.conv.2.weight', 'module.conv.2.bias', 'module.conv.2.running_mean', 'module.conv.2.running_var', 'module.conv.2.num_batches_tracked', 'module.conv.3.weight', 'module.conv.3.bias', 'module.conv.5.weight', 'module.conv.5.bias', 'module.conv.5.running_mean', 'module.conv.5.running_var', 'module.conv.5.num_batches_tracked', 'module.conv.6.weight', 'module.conv.6.bias', 'module.conv.8.weight', 'module.conv.8.bias', 'module.conv.8.running_mean', 'module.conv.8.running_var', 'module.conv.8.num_batches_tracked', 'module.conv.9.weight', 'module.conv.9.bias', 'module.conv.11.weight', 'module.conv.11.bias', 'module.conv.11.running_mean', 'module.conv.11.running_var', 'module.conv.11.num_batches_tracked'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_inf = nn.DataParallel(AudioClassifier())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_inf = model_inf.to(device)\n",
        "model_inf.load_state_dict(checkpoint)\n",
        "model_inf.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFjea0b5hIQB",
        "outputId": "72c0da1f-b211-4a7f-b156-69730f222197"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): AudioClassifier(\n",
              "    (conv1): Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "    (relu1): ReLU()\n",
              "    (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (relu2): ReLU()\n",
              "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (relu3): ReLU()\n",
              "    (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (relu4): ReLU()\n",
              "    (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (ap): AdaptiveAvgPool2d(output_size=1)\n",
              "    (lin): Linear(in_features=64, out_features=10, bias=True)\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "      (1): ReLU()\n",
              "      (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (4): ReLU()\n",
              "      (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (7): ReLU()\n",
              "      (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (9): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (10): ReLU()\n",
              "      (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference(model_inf, val_dl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfTRIKsUlpvg",
        "outputId": "d7d9a917-dc27-4ec4-8de7-118e23621593"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.83, Total items: 1781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hO-gPDjyiROX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
>>>>>>> 011613e4836bae421ee973e065cd006db8bc5cc2
